{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ensemble_code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "먼저, github 내부의 다른 code를 통해서 모델들을 각각 train한 후, 얻은 checkpoint가 필요하다."
      ],
      "metadata": {
        "id": "bdmAOGQO116-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KEXmL-Va-wRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install simpletransformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "rvuKc779-bDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import (\n",
        "    BertForSequenceClassification,\n",
        "    BertTokenizer,\n",
        "    AutoConfig,\n",
        "    AdamW,\n",
        "    ElectraForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    XLNetForSequenceClassification,\n",
        "    XLNetTokenizer,\n",
        "    XLNetModel,\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        "    RobertaModel,\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "3wkrYu9l-cWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "Vc3EbV8NwVba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **test data**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lkevM7Wy6bcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_df = pd.read_csv('test_no_label.csv')"
      ],
      "metadata": {
        "id": "w-Q7GSXD57Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = test_df['Id']"
      ],
      "metadata": {
        "id": "Gi50frWm5802"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_id_file_test(tokenizer, test_dataset):\n",
        "    data_strings = []\n",
        "    id_file_data = [tokenizer.encode(sent.lower()) for sent in test_dataset]\n",
        "    for item in id_file_data:\n",
        "        data_strings.append(' '.join([str(k) for k in item]))\n",
        "    return data_strings"
      ],
      "metadata": {
        "id": "Exd9rb9W59gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = make_id_file_test(tokenizer, test_dataset)"
      ],
      "metadata": {
        "id": "c_OfUi8T5-mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[:10]"
      ],
      "metadata": {
        "id": "Lo_YszAw6CYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentTestDataset(object):\n",
        "    def __init__(self, tokenizer, test):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = []\n",
        "\n",
        "        for sent in test:\n",
        "            self.data += [self._cast_to_int(sent.strip().split())]\n",
        "\n",
        "    def _cast_to_int(self, sample):\n",
        "        return [int(word_id) for word_id in sample]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.data[index]\n",
        "        return np.array(sample)"
      ],
      "metadata": {
        "id": "nk2QxUDb6C95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SentimentTestDataset(tokenizer, test)"
      ],
      "metadata": {
        "id": "daHO5TIs6Qft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_style_test(samples):\n",
        "    input_ids = samples\n",
        "    print(input_ids)\n",
        "    max_len = max(len(input_id) for input_id in input_ids)\n",
        "    # error code\n",
        "    #sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1] \n",
        "    \n",
        "    # train을 할 때와는 달리, test는 label이 없기 때문에 shuffle을 해주면, 각각의 순서가 맞지 않는다. \n",
        "    # shuffle을 하지 않아야 하므로, index 값으로 다시 코드 작성을 해주어야 한다. \n",
        "    sorted_indices = list(i for i in range(len(input_ids))) \n",
        "\n",
        "    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n",
        "                             batch_first=True)\n",
        "    attention_mask = torch.tensor(\n",
        "        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n",
        "         sorted_indices])\n",
        "    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n",
        "    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids, position_ids"
      ],
      "metadata": {
        "id": "eL65v30r6RPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch_size = 64\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n",
        "                                          shuffle=False, collate_fn=collate_fn_style_test,\n",
        "                                          num_workers=2)"
      ],
      "metadata": {
        "id": "eKj6rHQV6Swy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. bert base uncased**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UM8g5HQ35QwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/NLP/project/checkpoint_epoch_1.13850_model1.pth'\n",
        "# 해당하는 모델을 입력한다.\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "# 앞의 train 부분에서의 check point의 파일경로를 삽입한다. \n",
        "checkpoint1 = torch.load('/content/drive/MyDrive/Colab Notebooks/NLP/project/0/checkpoint_epoch_0.5536.pth', map_location=device)\n",
        "model.load_state_dict(checkpoint1['model_state_dict'])\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predictions1 = []\n",
        "    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n",
        "                                                                        desc='Test',\n",
        "                                                                        position=1,\n",
        "                                                                        leave=None):\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        position_ids = position_ids.to(device)\n",
        "\n",
        "        output1 = model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask,\n",
        "                       token_type_ids=token_type_ids,\n",
        "                       position_ids=position_ids)\n",
        "\n",
        "        logits1 = output1.logits\n",
        "        batch_predictions1 = [0 if example[0] > example[1] else 1 for example in logits1]\n",
        "        # 모델이 예측한 결과 값\n",
        "        predictions1 += batch_predictions1"
      ],
      "metadata": {
        "id": "eU8pPBU15sw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. bert large**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4BtlH6ub61jD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejF189dw4Zj1"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/NLP/project/checkpoint_epoch_1.13850.pth'\n",
        "# 해당하는 모델을 입력한다.\n",
        "model = BertForSequenceClassification.from_pretrained('bert-large-uncased')\n",
        "# 앞의 train 부분에서의 check point의 파일경로를 삽입한다. \n",
        "checkpoint2 = torch.load('/content/drive/MyDrive/Colab Notebooks/NLP/project/0/checkpoint_epoch_0.5536.pth', map_location=device)\n",
        "model.load_state_dict(checkpoint2['model_state_dict'])\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predictions2 = []\n",
        "    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n",
        "                                                                        desc='Test',\n",
        "                                                                        position=1,\n",
        "                                                                        leave=None):\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        position_ids = position_ids.to(device)\n",
        "\n",
        "        output2 = model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask,\n",
        "                       token_type_ids=token_type_ids,\n",
        "                       position_ids=position_ids)\n",
        "\n",
        "        logits2 = output2.logits\n",
        "        batch_predictions2 = [0 if example[0] > example[1] else 1 for example in logits2]\n",
        "        # 모델이 예측한 결과 값\n",
        "        predictions2 += batch_predictions2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Electra**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tmzN7EnF7GX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/NLP/project/checkpoint_epoch_1.13850.pth'\n",
        "# 해당하는 모델을 입력한다.\n",
        "model = ElectraForSequenceClassification.from_pretrained(\"google/electra-base-discriminator\")\n",
        "# 앞의 train 부분에서의 check point의 파일경로를 삽입한다. \n",
        "checkpoint3 = torch.load('/content/drive/MyDrive/Colab Notebooks/NLP/project/1/checkpoint_epoch_1.6228.pth', map_location=device)\n",
        "model.load_state_dict(checkpoint3['model_state_dict'])\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predictions3 = []\n",
        "    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n",
        "                                                                        desc='Test',\n",
        "                                                                        position=1,\n",
        "                                                                        leave=None):\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        position_ids = position_ids.to(device)\n",
        "\n",
        "        output3 = model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask,\n",
        "                       token_type_ids=token_type_ids,\n",
        "                       position_ids=position_ids)\n",
        "\n",
        "        logits3 = output3.logits\n",
        "        batch_predictions3 = [0 if example[0] > example[1] else 1 for example in logits3]\n",
        "        # 모델이 예측한 결과 값\n",
        "        predictions3 += batch_predictions3"
      ],
      "metadata": {
        "id": "C5ndFava7Yli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. XLNet**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KrzR92z07LIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/NLP/project/checkpoint_epoch_1.13850.pth'\n",
        "# 해당하는 모델을 입력한다.\n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased')\n",
        "# 앞의 train 부분에서의 check point의 파일경로를 삽입한다. \n",
        "checkpoint4 = torch.load('/content/drive/MyDrive/Colab Notebooks/NLP/project/2/checkpoint_epoch_1.4152.pth', map_location=device)\n",
        "model.load_state_dict(checkpoint4['model_state_dict'])\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predictions4 = []\n",
        "    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n",
        "                                                                        desc='Test',\n",
        "                                                                        position=1,\n",
        "                                                                        leave=None):\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        position_ids = position_ids.to(device)\n",
        "\n",
        "        output4 = model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask,\n",
        "                       token_type_ids=token_type_ids,\n",
        "                       position_ids=position_ids)\n",
        "        \n",
        "\n",
        "\n",
        "        logits4 = output4.logits\n",
        "\n",
        "        batch_predictions4 = [0 if example[0] > example[1] else 1 for example in logits4]\n",
        "        # 모델이 예측한 결과 값\n",
        "        predictions4 += batch_predictions4"
      ],
      "metadata": {
        "id": "jwxIrdmu7W7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Roberta**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6s_JpinJ7OHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/NLP/project/check_roberta/checkpoint_epoch_1.13850.pth'\n",
        "# 해당하는 모델을 입력한다.\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "# 앞의 train 부분에서의 check point의 파일경로를 삽입한다. \n",
        "checkpoint5 = torch.load('/content/drive/MyDrive/Colab Notebooks/NLP/project/check_roberta/checkpoint_epoch_1.1730.pth', map_location=device)\n",
        "model.load_state_dict(checkpoint5['model_state_dict'])\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predictions5 = []\n",
        "    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n",
        "                                                                        desc='Test',\n",
        "                                                                        position=1,\n",
        "                                                                        leave=None):\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        position_ids = position_ids.to(device)\n",
        "\n",
        "        output5 = model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask,\n",
        "                       token_type_ids=token_type_ids,\n",
        "                       position_ids=position_ids)\n",
        "\n",
        "        logits5 = output5.logits\n",
        "        batch_predictions5 = [0 if example[0] > example[1] else 1 for example in logits5]\n",
        "        # 모델이 예측한 결과 값\n",
        "        predictions5 += batch_predictions5"
      ],
      "metadata": {
        "id": "gOjh-yYg8Gsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble**\n",
        "\n",
        "---\n",
        "\n",
        "ensemble 방법으로는 hard-voting을 사용하였다. 각각의 결과 값에 대해서 각 모델들의 예측 결과 값을 다 더한 후 모델의 개수만큼 나누어 주었다. 이 값이 0.5 이상이라면 1이라고 판단, 0.5 미만이라면 0이라고 판단하였다.\n",
        "이러한 방법을 통해 최종적인 예측 값을 낼 수 있다."
      ],
      "metadata": {
        "id": "AfFWWL9P8JFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for i in range(len(predictions1)):\n",
        "    new = ((int(predictions1[i]) + int(predictions2[i]) + int(predictions3[i]) + int(predictions4[i]) + int(predictions5[i])) / 5)\n",
        "    if new > 0.5:\n",
        "        predictions.append(1)\n",
        "    else:\n",
        "        predictions.append(0)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "xq45Yyi_8LT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "p90P88G87ldk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['Category'] = predictions"
      ],
      "metadata": {
        "id": "eeZJa0y77jeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv('submission1.csv', index=False)"
      ],
      "metadata": {
        "id": "3lJnvB597k5Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}