{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "baseline-xlnet-base-cased",
      "provenance": [],
      "collapsed_sections": [],
      "background_execution": "on"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import requirements"
      ],
      "metadata": {
        "id": "TH32rzgprvgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pzkvQI7VKTrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpletransformers"
      ],
      "metadata": {
        "id": "RaJEPKVgCG6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import (\n",
        "    XLNetForSequenceClassification,\n",
        "    XLNetTokenizer,\n",
        "    XLNetModel,\n",
        "    AutoConfig,\n",
        "    AdamW\n",
        ")"
      ],
      "metadata": {
        "id": "AAdLxrUZrvgP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocess"
      ],
      "metadata": {
        "id": "ASWOOmXqrvgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_id_file(task, tokenizer):\n",
        "    def make_data_strings(file_name):\n",
        "        data_strings = []\n",
        "        with open(os.path.join(file_name), 'r', encoding='utf-8') as f:\n",
        "            id_file_data = [tokenizer.encode(line.lower()) for line in f.readlines()]\n",
        "        for item in id_file_data:\n",
        "            data_strings.append(' '.join([str(k) for k in item]))\n",
        "        return data_strings\n",
        "    \n",
        "    print('it will take some times...')\n",
        "    train_pos = make_data_strings('sentiment.train.1')\n",
        "    train_neg = make_data_strings('sentiment.train.0')\n",
        "    dev_pos = make_data_strings('sentiment.dev.1')\n",
        "    dev_neg = make_data_strings('sentiment.dev.0')\n",
        "\n",
        "    print('make id file finished!')\n",
        "    return train_pos, train_neg, dev_pos, dev_neg"
      ],
      "metadata": {
        "id": "RAnU6w29rvgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')"
      ],
      "metadata": {
        "id": "Ui2HOCflrvgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "4jVuK-V1uq3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "ttzRlY4Ov0jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos, train_neg, dev_pos, dev_neg = make_id_file('yelp', tokenizer)"
      ],
      "metadata": {
        "id": "BAgztXIBrvgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos[:10]"
      ],
      "metadata": {
        "id": "wRh2WjGRrvgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(object):\n",
        "    def __init__(self, tokenizer, pos, neg):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = []\n",
        "        self.label = []\n",
        "\n",
        "        for pos_sent in pos:\n",
        "            self.data += [self._cast_to_int(pos_sent.strip().split())]\n",
        "            self.label += [[1]]\n",
        "        for neg_sent in neg:\n",
        "            self.data += [self._cast_to_int(neg_sent.strip().split())]\n",
        "            self.label += [[0]]\n",
        "\n",
        "    def _cast_to_int(self, sample):\n",
        "        return [int(word_id) for word_id in sample]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.data[index]\n",
        "        return np.array(sample), np.array(self.label[index])"
      ],
      "metadata": {
        "id": "JdpQQQMUrvgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SentimentDataset(tokenizer, train_pos, train_neg)\n",
        "dev_dataset = SentimentDataset(tokenizer, dev_pos, dev_neg)"
      ],
      "metadata": {
        "id": "wCz5ey8xrvgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(train_dataset):\n",
        "    print(item)\n",
        "    if i == 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "UuvkMczvrvgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_style(samples):\n",
        "    input_ids, labels = zip(*samples)\n",
        "    max_len = max(len(input_id) for input_id in input_ids)\n",
        "    sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n",
        "\n",
        "    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n",
        "                             batch_first=True)\n",
        "    attention_mask = torch.tensor(\n",
        "        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n",
        "         sorted_indices])\n",
        "    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n",
        "    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n",
        "    labels = torch.tensor(np.stack(labels, axis=0)[sorted_indices])\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids, position_ids, labels"
      ],
      "metadata": {
        "id": "B0wRUBYSrvgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random seed\n",
        "random_seed=42\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "oXk3v4B2ItSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_acc(predictions, target_labels):\n",
        "    return (np.array(predictions) == np.array(target_labels)).mean()"
      ],
      "metadata": {
        "id": "cjAVHyfkIuli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "    def __init__(self, device, output_path, lr, resume_path):\n",
        "        self.output_path = output_path\n",
        "        self.device = device\n",
        "        self.model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased')\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=lr)\n",
        "        \n",
        "        #For learning curve\n",
        "        self.training_stats = []\n",
        "        if resume_path :\n",
        "            checkpoint = torch.load(resume_path, map_location=device)\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.start_train_epoch = checkpoint['epoch'] + 1\n",
        "            self.lowest_valid_loss = checkpoint['lowest_valid_loss']\n",
        "        else:\n",
        "            self.start_train_epoch = 0\n",
        "            self.lowest_valid_loss = 9999.\n",
        "        self.model.to(self.device)\n",
        "        for state in self.optimizer.state.values():\n",
        "            for k, v in state.items():\n",
        "                if torch.is_tensor(v):\n",
        "                    state[k] = v.to(device)\n",
        "\n",
        "    def training(self, train_loader, dev_loader, last_epoch):\n",
        "        self.model.train()\n",
        "        for epoch in range(self.start_train_epoch, last_epoch):\n",
        "            with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "                for iteration, (input_ids, attention_mask, token_type_ids, position_ids, labels) in enumerate(tepoch):\n",
        "                    \n",
        "                    tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                    input_ids = input_ids.to(self.device)\n",
        "                    attention_mask = attention_mask.to(self.device)\n",
        "                    token_type_ids = token_type_ids.to(self.device)\n",
        "                    position_ids = position_ids.to(self.device)\n",
        "                    labels = labels.to(self.device, dtype=torch.long)\n",
        "\n",
        "                    self.optimizer.zero_grad()\n",
        "\n",
        "                    output = self.model(input_ids=input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               labels=labels)\n",
        "\n",
        "                    loss = output.loss\n",
        "                    loss.backward()\n",
        "\n",
        "                    self.optimizer.step()\n",
        "                    \n",
        "                \n",
        "                    tepoch.set_postfix(loss=loss.item())\n",
        "                    \n",
        "                    if iteration != 0 and iteration % int(len(train_loader) / 10) == 0:\n",
        "                        # Evaluate the model five times per epoch\n",
        "                        \n",
        "                        with torch.no_grad():\n",
        "\n",
        "                            self.model.eval()\n",
        "                            valid_losses = []\n",
        "                            predictions = []\n",
        "                            target_labels = []\n",
        "                            for input_ids, attention_mask, token_type_ids, position_ids, labels in tqdm(dev_loader,\n",
        "                                                                                                    desc='Eval',\n",
        "                                                                                                    position=1,\n",
        "                                                                                                    leave=None):\n",
        "                                \n",
        "                                \n",
        "                                input_ids = input_ids.to(self.device)\n",
        "                                attention_mask = attention_mask.to(self.device)\n",
        "                                token_type_ids = token_type_ids.to(self.device)\n",
        "                                position_ids = position_ids.to(self.device)\n",
        "                                labels = labels.to(self.device, dtype=torch.long)\n",
        "\n",
        "                                output = self.model(input_ids=input_ids,\n",
        "                                           attention_mask=attention_mask,\n",
        "                                           token_type_ids=token_type_ids,\n",
        "                                           position_ids=position_ids,\n",
        "                                           labels=labels)\n",
        "\n",
        "                                logits = output.logits\n",
        "                                loss = output.loss\n",
        "                                valid_losses.append(loss.item())\n",
        "\n",
        "                                batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n",
        "                                batch_labels = [int(example) for example in labels]\n",
        "\n",
        "                                predictions += batch_predictions\n",
        "                                target_labels += batch_labels\n",
        "\n",
        "                        print(epoch)\n",
        "                        acc = compute_acc(predictions, target_labels)\n",
        "                        valid_loss = sum(valid_losses) / len(valid_losses)\n",
        "                        \n",
        "                        # For Learning curve  \n",
        "                        #각 epoch의 iteration 마다 Train,Validation의 Loss 와 accuracy 를 저장\n",
        "                        self.training_stats.append( {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'iteration' : iteration,\n",
        "                                'Training Loss': loss.item(),\n",
        "                                'Valid Loss': valid_loss,\n",
        "                                'Valid Accuracy': acc})\n",
        "                        if self.lowest_valid_loss > valid_loss:\n",
        "                            print('')\n",
        "                            print('Acc for model which have lower valid loss: ', acc)\n",
        "                            #torch.save(self.model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/NLP/project/pytorch_model.bin\")\n",
        "                            torch.save({\n",
        "                                'epoch': epoch,\n",
        "                                'lowest_valid_loss': self.lowest_valid_loss,\n",
        "                                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                                'model_state_dict': self.model.state_dict(),\n",
        "                                }, f'{self.output_path}/checkpoint_epoch_{epoch}.{iteration}.pth')\n",
        "                            self.lowest_valid_loss = valid_loss\n",
        "                            print('--------------save checkpoint at epoch : {}--------------'.format(epoch))\n",
        "                            print('--------------lowest_valid_loss : {}--------------'.format(self.lowest_valid_loss))"
      ],
      "metadata": {
        "id": "pAZTHhj6IwUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size=32\n",
        "eval_batch_size=32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=train_batch_size,\n",
        "                                           shuffle=True, collate_fn=collate_fn_style,\n",
        "                                           pin_memory=True, num_workers=2)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=eval_batch_size,\n",
        "                                         shuffle=False, collate_fn=collate_fn_style,\n",
        "                                         num_workers=2)\n",
        "\n",
        "# 경로설정\n",
        "output_path = '/content/drive/MyDrive/Colab Notebooks/NLP/project/checkpoints'\n",
        "resume_path = None \n",
        "\n",
        "lr = 5e-5 \n",
        "last_epoch = 5\n",
        "\n",
        "trainer = Trainer(device,output_path,lr,resume_path)\n",
        "trainer.training(train_loader, dev_loader, last_epoch)"
      ],
      "metadata": {
        "id": "5saagig0rvgV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}